{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework # 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primal versus Dual Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "\n",
    "Recall that $N$ is the size of the data set and $d$ is the dimensionality of the\n",
    "input space. The original formulation of the hard-margin SVM problem (minimize\n",
    "$\\frac{1}{2}\\mathbf{w}^\\intercal\\mathbf{w}$ subject to the inequality constraints), without going through the\n",
    "Lagrangian dual problem, is\n",
    "\n",
    "[a] a quadratic programming problem with $N$ variables\n",
    "\n",
    "[b] a quadratic programming problem with $N + 1$ variables\n",
    "\n",
    "[c] a quadratic programming problem with $d$ variables\n",
    "\n",
    "[d] a quadratic programming problem with $d + 1$ variables\n",
    "\n",
    "[e] not a quadratic programming problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem has $N$ variable in $\\mathbf{w}$ and one variable $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: The following problems deal with a real-life data set. In addition, the computational packages you use may employ different heuristics and require different tweaks.\n",
    "This is a typical situation that a Machine Learning practitioner faces. There are\n",
    "uncertainties, and the answers may or may not match our expectations. Although\n",
    "this situation is not as ‘sanitized’ as other homework problems, it is important to go\n",
    "through it as part of the learning experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Soft Margins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rest of the problems of this homework set, we apply soft-margin SVM to\n",
    "handwritten digits from the processed US Postal Service Zip Code data set. Download\n",
    "the data (extracted features of intensity and symmetry) for training and testing:\n",
    "\n",
    "http://www.amlbook.com/data/zip/features.train\n",
    "\n",
    "http://www.amlbook.com/data/zip/features.test\n",
    "\n",
    "(the format of each row is: digit intensity symmetry). We will train two types\n",
    "of binary classifiers; one-versus-one (one digit is class $+1$ and another digit is class\n",
    "$−1$, with the rest of the digits disregarded), and one-versus-all (one digit is class $+1$\n",
    "and the rest of the digits are class $−1$).\n",
    "\n",
    "The data set has thousands of points, and some quadratic programming packages\n",
    "cannot handle this size. We recommend that you use the packages in libsvm:\n",
    "\n",
    "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
    "\n",
    "Implement SVM with soft margin on the above zip-code data set by solving\n",
    "$$\\min \\frac{1}{2}\\sum_{n=1}^N \\sum_{m=1}^N \\alpha_n \\alpha_m y_n y_m K(\\mathbf{x}_n, \\mathbf{x}_m) - \\sum_{n=1}^N \\alpha_n$$\n",
    "$$\\begin{eqnarray}\n",
    "\\mbox{s.t. } & &\\sum_{n=1}^N y_n \\alpha_n = 0 \\\\\n",
    "& &0 \\leq \\alpha_n \\leq C, n = 1, \\cdots, N\n",
    "\\end{eqnarray}$$\n",
    "When evaluating $E_\\mathrm{in}$ and $E_\\mathrm{out}$ of the resulting classifier, use binary classification error.\n",
    "\n",
    "Practical remarks:\n",
    "\n",
    "(i) For the purpose of this homework, do not scale the data when you use libsvm or\n",
    "other packages, otherwise you may inadvertently change the (effective) kernel and get\n",
    "different results.\n",
    "\n",
    "(ii) In some packages, you need to specify double precision.\n",
    "\n",
    "(iii) In 10-fold cross validation, if the data size is not a multiple of 10, the sizes of the\n",
    "10 subsets may be off by 1 data point.\n",
    "\n",
    "(iv) Some packages have software parameters whose values affect the outcome. ML\n",
    "practitioners have to deal with this kind of added uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filter_m_vs_n (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download(\"http://work.caltech.edu/data/features.train\", \"features.train\")\n",
    "# download(\"http://work.caltech.edu/data/features.test\", \"features.test\")\n",
    "\n",
    "training_data = readdlm(\"features.train\")\n",
    "test_data = readdlm(\"features.test\");\n",
    "\n",
    "train_features = training_data[:,2:3]\n",
    "train_labels = training_data[:,1]\n",
    "test_features = test_data[:,2:3]\n",
    "test_labels = test_data[:,1]\n",
    "\n",
    "function n_vs_all(labels, n)\n",
    "    nlabels = labels[:]\n",
    "    nlabels[labels .== n] = 1\n",
    "    nlabels[labels .!= n] = -1\n",
    "    return nlabels\n",
    "end\n",
    "\n",
    "function filter_m_vs_n(features, labels, m, n)\n",
    "    mids = ifelse(labels .== m, true, false)\n",
    "    nids = ifelse(labels .== n, true, false)\n",
    "    mlabels = 1*ones(sum(mids))\n",
    "    nlabels = -1*ones(sum(nids))\n",
    "    return [features[mids,:];features[nids,:]], [mlabels; nlabels]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernels\n",
    "\n",
    "Consider the polynomial kernel $K(\\mathbf{x}_n ,\\mathbf{x}_m) = (1 + \\mathbf{x}_n^\\intercal \\mathbf{x}_m)^Q$, \n",
    "where $Q$ is the degree of the polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "With $C = 0.01$ and $Q = 2$, which of the following classifiers has the highest\n",
    "$E_\\mathrm{in}$?\n",
    "\n",
    "[a] 0 versus all\n",
    "\n",
    "[b] 2 versus all\n",
    "\n",
    "[c] 4 versus all\n",
    "\n",
    "[d] 6 versus all\n",
    "\n",
    "[e] 8 versus all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.10588396653408316\n",
      "2: 0.10026059525442321\n",
      "4: 0.08942531888629812\n",
      "6: 0.09107118365107669\n",
      "8: 0.074338225209162\n"
     ]
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "@sk_import svm: SVC\n",
    "\n",
    "function question2()\n",
    "    for i in 0:2:8\n",
    "        labels = n_vs_all(train_labels, i)\n",
    "        clf = SVC(C=0.01, kernel=\"poly\", degree=2, gamma=1.0, coef0=1.0)\n",
    "        fit!(clf, train_features, labels)\n",
    "        println(i, \": \", 1.0 - score(clf, train_features, labels))\n",
    "    end\n",
    "end\n",
    "\n",
    "question2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "With $C = 0.01$ and $Q = 2$, which of the following classifiers has the lowest\n",
    "$E_\\mathrm{in}$?\n",
    "\n",
    "[a] 1 versus all\n",
    "\n",
    "[b] 3 versus all\n",
    "\n",
    "[c] 5 versus all\n",
    "\n",
    "[d] 7 versus all\n",
    "\n",
    "[e] 9 versus all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.014401316691811772\n",
      "3: 0.09024825126868741\n",
      "5: 0.07625840076807022\n",
      "7: 0.08846523110684401\n",
      "9: 0.08832807570977919\n"
     ]
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "@sk_import svm: SVC\n",
    "\n",
    "function question3()\n",
    "    for i in 1:2:9\n",
    "        labels = n_vs_all(train_labels, i)\n",
    "        clf = SVC(C=0.01, kernel=\"poly\", degree=2, gamma=1.0, coef0=1.0)\n",
    "        fit!(clf, train_features, labels)\n",
    "        println(i, \": \", 1.0 - score(clf, train_features, labels))\n",
    "    end\n",
    "end\n",
    "\n",
    "question3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "Comparing the two selected classifiers from Problems 2 and 3, which of the\n",
    "following values is the closest to the difference between the number of support\n",
    "vectors of these two classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1793"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "@sk_import svm: SVC\n",
    "\n",
    "function question4()\n",
    "    labels0 = n_vs_all(train_labels, 0)\n",
    "    labels1 = n_vs_all(train_labels, 1)\n",
    "    clf = SVC(C=0.01, kernel=\"poly\", degree=2, gamma=1.0, coef0=1.0)\n",
    "    fit!(clf, train_features, labels0)\n",
    "    len0 = length(clf[:support_])\n",
    "    fit!(clf, train_features, labels1)\n",
    "    len1 = length(clf[:support_])\n",
    "    return abs(len0 - len1)\n",
    "end\n",
    "\n",
    "question4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "Consider the 1 versus 5 classifier with $Q = 2$ and $C \\in \\{0.001,0.01,0.1,1\\}$.\n",
    "Which of the following statements is correct? Going up or down means strictly\n",
    "so.\n",
    "\n",
    "[a] The number of support vectors goes down when $C$ goes up.\n",
    "\n",
    "[b] The number of support vectors goes up when $C$ goes up.\n",
    "\n",
    "[c] $E_\\mathrm{out}$ goes down when $$ goes up.\n",
    "\n",
    "[d] Maximum $C$ achieves the lowest $E_\\mathrm{in}$.\n",
    "\n",
    "[e] None of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:0.001\n",
      "# of sv:76\n",
      "E_in:0.004484304932735439\n",
      "E_out:0.01650943396226412\n",
      "---\n",
      "C:0.01\n",
      "# of sv:34\n",
      "E_in:0.004484304932735439\n",
      "E_out:0.018867924528301883\n",
      "---\n",
      "C:0.1\n",
      "# of sv:24\n",
      "E_in:0.004484304932735439\n",
      "E_out:0.018867924528301883\n",
      "---\n",
      "C:1.0\n",
      "# of sv:24\n",
      "E_in:0.0032030749519538215\n",
      "E_out:0.018867924528301883\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "@sk_import svm: SVC\n",
    "\n",
    "function question5()\n",
    "    features, labels = filter_m_vs_n(train_features, train_labels, 1, 5)\n",
    "    t_features, t_labels = filter_m_vs_n(test_features, test_labels, 1, 5)\n",
    "    for c in [0.001,0.01,0.1,1]\n",
    "        clf = SVC(C=c, kernel=\"poly\", degree=2, gamma=1.0, coef0=1.0)\n",
    "        fit!(clf, features, labels)\n",
    "        println(\"C:\",c)\n",
    "        println(\"# of sv:\", length(clf[:support_]))\n",
    "        println(\"E_in:\", 1 - score(clf, features, labels))\n",
    "        println(\"E_out:\", 1 - score(clf, t_features, t_labels))\n",
    "        println(\"---\")\n",
    "    end\n",
    "end\n",
    "\n",
    "question5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "In the 1 versus 5 classifier, comparing $Q = 2$ with $Q = 5$, which of the following\n",
    "statements is correct?\n",
    "\n",
    "[a] When $C = 0.0001$, $E_\\mathrm{in}$ is higher at $Q = 5$.\n",
    "\n",
    "[b] When $C = 0.001$, the number of support vectors is lower at $Q = 5$.\n",
    "\n",
    "[c] When $C = 0.01$, $E_\\mathrm{in}$ is higher at $Q = 5$.\n",
    "\n",
    "[d] When $C = 1$, $E_\\mathrm{out}$ is lower at $Q = 5$.\n",
    "\n",
    "[e] None of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:0.0001\n",
      "# of sv:236\n",
      "E_in:0.008968609865470878\n",
      "E_out:0.01650943396226412\n",
      "---\n",
      "C:0.0001\n",
      "# of sv:26\n",
      "E_in:0.004484304932735439\n",
      "E_out:0.018867924528301883\n",
      "---\n",
      "C:0.001\n",
      "# of sv:76\n",
      "E_in:0.004484304932735439\n",
      "E_out:0.01650943396226412\n",
      "---\n",
      "C:0.001\n",
      "# of sv:25\n",
      "E_in:0.004484304932735439\n",
      "E_out:0.021226415094339646\n",
      "---\n",
      "C:0.01\n",
      "# of sv:34\n",
      "E_in:0.004484304932735439\n",
      "E_out:0.018867924528301883\n",
      "---\n",
      "C:0.01\n",
      "# of sv:23\n",
      "E_in:0.0038436899423446302\n",
      "E_out:0.021226415094339646\n",
      "---\n",
      "C:1.0\n",
      "# of sv:24\n",
      "E_in:0.0032030749519538215\n",
      "E_out:0.018867924528301883\n",
      "---\n",
      "C:1.0\n",
      "# of sv:21\n",
      "E_in:0.0032030749519538215\n",
      "E_out:0.021226415094339646\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "@sk_import svm: SVC\n",
    "\n",
    "function question6()\n",
    "    features, labels = filter_m_vs_n(train_features, train_labels, 1, 5)\n",
    "    t_features, t_labels = filter_m_vs_n(test_features, test_labels, 1, 5)\n",
    "    for c in [0.0001, 0.001, 0.01, 1]\n",
    "        for q in [2, 5]\n",
    "                clf = SVC(C=c, kernel=\"poly\", degree=q, gamma=1.0, coef0=1.0)\n",
    "            fit!(clf, features, labels)\n",
    "            println(\"C:\",c)\n",
    "            println(\"# of sv:\", length(clf[:support_]))\n",
    "            println(\"E_in:\", 1 - score(clf, features, labels))\n",
    "            println(\"E_out:\", 1 - score(clf, t_features, t_labels))\n",
    "            println(\"---\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "question6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cross Validation\n",
    "In the next two problems, we will experiment with 10-fold cross validation for the\n",
    "polynomial kernel. Because E cv is a random variable that depends on the random\n",
    "partition of the data, we will try 100 runs with different partitions and base our\n",
    "answer on how many runs lead to a particular choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\n",
    "Consider the 1 versus 5 classifier with $Q = 2$. We use $E_\\mathrm{cv}$ to select $C \\in \\{0.0001,0.001,0.01,0.1,1\\}$.\n",
    "If there is a tie in $E\\mathrm{cv}$, select the smaller $C$. Within the 100 random runs, which of the following statements is correct?\n",
    "\n",
    "[a] $C = 0.0001$ is selected most often.\n",
    "\n",
    "[b] $C = 0.001$ is selected most often.\n",
    "\n",
    "[c] $C = 0.01$ is selected most often.\n",
    "\n",
    "[d] $C = 0.1$ is selected most often.\n",
    "\n",
    "[e] $C = 1$ is selected most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict{Any,Any}(4=>14,2=>51,3=>24,5=>11)\n"
     ]
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "using ScikitLearn.CrossValidation: KFold\n",
    "@sk_import svm: SVC\n",
    "\n",
    "function question7()\n",
    "    features, labels = filter_m_vs_n(train_features, train_labels, 1, 5)\n",
    "    dist = Dict()\n",
    "    for _ in 1:100\n",
    "        kfold = KFold(length(labels), n_folds=10, shuffle=true)\n",
    "        err_cnts = []\n",
    "        for c in [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "            clf = SVC(C=c, kernel=\"poly\", degree=2, gamma=1.0, coef0=1.0)\n",
    "            err_cnt = 0\n",
    "            for (train, valid) in kfold\n",
    "                fit!(clf, features[train,:], labels[train])\n",
    "                err_cnt += sum(predict(clf, features[valid,:]) .!= labels[valid])\n",
    "            end\n",
    "            push!(err_cnts, err_cnt)\n",
    "        end\n",
    "        _, winner = findmin(err_cnts)\n",
    "        dist[winner] = get(dist, winner, 0) + 1\n",
    "    end\n",
    "    println(dist)\n",
    "end\n",
    "\n",
    "question7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.\n",
    "Again, consider the 1 versus 5 classifier with $Q = 2$. For the winning selection\n",
    "in the previous problem, the average value of $E_\\mathrm{cv}$ over the 100 runs is closest to\n",
    "\n",
    "[a] 0.001\n",
    "\n",
    "[b] 0.003\n",
    "\n",
    "[c] 0.005\n",
    "\n",
    "[d] 0.007\n",
    "\n",
    "[e] 0.009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004798206278026906\n"
     ]
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "using ScikitLearn.CrossValidation: KFold\n",
    "@sk_import svm: SVC\n",
    "\n",
    "function question8()\n",
    "    features, labels = filter_m_vs_n(train_features, train_labels, 1, 5)\n",
    "    err_cnts = []\n",
    "    clf = SVC(C=0.001, kernel=\"poly\", degree=2, gamma=1.0, coef0=1.0)\n",
    "    for _ in 1:100\n",
    "        kfold = KFold(length(labels), n_folds=10, shuffle=true)\n",
    "        err_cnt = 0\n",
    "        for (train, valid) in kfold\n",
    "            fit!(clf, features[train,:], labels[train])\n",
    "            err_cnt += sum(predict(clf, features[valid,:]) .!= labels[valid])\n",
    "        end\n",
    "        push!(err_cnts, err_cnt)\n",
    "    end\n",
    "    println(mean(err_cnts) / length(labels))\n",
    "end\n",
    "\n",
    "question8()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF Kernel\n",
    "Consider the radial basis function (RBF) kernel $K(\\mathbf{x}_n,\\mathbf{x}_m) = \\exp(-\\|\\mathbf{x}_n − \\mathbf{x}_m\\|^2)$ in\n",
    "the soft-margin SVM approach. Focus on the 1 versus 5 classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.\n",
    "Which of the following values of $C$ results in the lowest E in ?\n",
    "\n",
    "[a] $C = 0.01$\n",
    "\n",
    "[b] $C = 1$\n",
    "\n",
    "[c] $C = 100$\n",
    "\n",
    "[d] $C = 10^4$\n",
    "\n",
    "[e] $C = 10^6$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.\n",
    "Which of the following values of $C$ results in the lowest $E_\\mathrm{out}$?\n",
    "[a] $C = 0.01$\n",
    "\n",
    "[b] $C = 1$\n",
    "\n",
    "[c] $C = 100$\n",
    "\n",
    "[d] $C = 10^4$\n",
    "\n",
    "[e] $C = 10^6$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:0.01\n",
      "# of sv:406\n",
      "E_in:0.0038436899423446302\n",
      "E_out:0.02358490566037741\n",
      "---\n",
      "C:1.0\n",
      "# of sv:31\n",
      "E_in:0.004484304932735439\n",
      "E_out:0.021226415094339646\n",
      "---\n",
      "C:100.0\n",
      "# of sv:22\n",
      "E_in:0.0032030749519538215\n",
      "E_out:0.018867924528301883\n",
      "---\n",
      "C:10000.0\n",
      "# of sv:19\n",
      "E_in:0.002562459961563124\n",
      "E_out:0.02358490566037741\n",
      "---\n",
      "C:1.0e6\n",
      "# of sv:17\n",
      "E_in:0.0006406149903908087\n",
      "E_out:0.02358490566037741\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "@sk_import svm: SVC\n",
    "\n",
    "function question9and10()\n",
    "    features, labels = filter_m_vs_n(train_features, train_labels, 1, 5)\n",
    "    t_features, t_labels = filter_m_vs_n(test_features, test_labels, 1, 5)\n",
    "    for c in [0.01, 1, 100, 1e4, 1e6]\n",
    "        clf = SVC(C=c, kernel=\"rbf\", gamma=1.0)\n",
    "        fit!(clf, features, labels)\n",
    "        println(\"C:\",c)\n",
    "        println(\"# of sv:\", length(clf[:support_]))\n",
    "        println(\"E_in:\", 1 - score(clf, features, labels))\n",
    "        println(\"E_out:\", 1 - score(clf, t_features, t_labels))\n",
    "        println(\"---\")\n",
    "    end\n",
    "end\n",
    "\n",
    "question9and10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendices A: LIBSVM.jl\n",
    "\n",
    "The implementation is not good and very slow. Furthermore, the API is terrible. So, it is not recommended to use this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pkg.clone(\"https://github.com/simonster/LIBSVM.jl\")\n",
    "#Pkg.build(\"LIBSVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using LIBSVM\n",
    "\n",
    "function question2()\n",
    "    for i in 0:2:8\n",
    "        labels = n_vs_all(train_labels, i)\n",
    "        clf = svmtrain(labels, train_features', kernel_type=Int32(1), degree=Int32(2), gamma=1.0, coef0=1.0)\n",
    "        predicted, _ = svmpredict(clf, train_features')\n",
    "        println(i, \": \",sum(predicted .!= labels) / length(labels))\n",
    "    end\n",
    "end\n",
    "question2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendices B: RCall.jl\n",
    "We can use R's functionality from Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RCall.RObject{RCall.StrSxp}\n",
       "[1] \"e1071\"     \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\" \n",
       "[7] \"methods\"   \"base\"     \n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using RCall\n",
    "R\"library(e1071)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RCall.RObject{RCall.StrSxp}\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm                   package:e1071                    R Documentation\n",
      "\n",
      "_\bS_\bu_\bp_\bp_\bo_\br_\bt _\bV_\be_\bc_\bt_\bo_\br _\bM_\ba_\bc_\bh_\bi_\bn_\be_\bs\n",
      "\n",
      "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
      "\n",
      "     ‘svm’ is used to train a support vector machine. It can be used\n",
      "     to carry out general regression and classification (of nu and\n",
      "     epsilon-type), as well as density-estimation. A formula interface\n",
      "     is provided.\n",
      "\n",
      "_\bU_\bs_\ba_\bg_\be:\n",
      "\n",
      "     ## S3 method for class 'formula'\n",
      "     svm(formula, data = NULL, ..., subset, na.action =\n",
      "     na.omit, scale = TRUE)\n",
      "     ## Default S3 method:\n",
      "     svm(x, y = NULL, scale = TRUE, type = NULL, kernel =\n",
      "     \"radial\", degree = 3, gamma = if (is.vector(x)) 1 else 1 / ncol(x),\n",
      "     coef0 = 0, cost = 1, nu = 0.5,\n",
      "     class.weights = NULL, cachesize = 40, tolerance = 0.001, epsilon = 0.1,\n",
      "     shrinking = TRUE, cross = 0, probability = FALSE, fitted = TRUE,\n",
      "     ..., subset, na.action = na.omit)\n",
      "     \n",
      "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
      "\n",
      " formula: a symbolic description of the model to be fit.\n",
      "\n",
      "    data: an optional data frame containing the variables in the model.\n",
      "          By default the variables are taken from the environment which\n",
      "          ‘svm’ is called from.\n",
      "\n",
      "       x: a data matrix, a vector, or a sparse matrix (object of class\n",
      "          ‘Matrix’ provided by the ‘Matrix’ package, or of class\n",
      "          ‘matrix.csr’ provided by the ‘SparseM’ package, or of\n",
      "          class ‘simple_triplet_matrix’ provided by the ‘slam’\n",
      "          package).\n",
      "\n",
      "       y: a response vector with one label for each row/component of\n",
      "          ‘x’. Can be either a factor (for classification tasks) or a\n",
      "          numeric vector (for regression).\n",
      "\n",
      "   scale: A logical vector indicating the variables to be scaled. If\n",
      "          ‘scale’ is of length 1, the value is recycled as many times\n",
      "          as needed.  Per default, data are scaled internally (both\n",
      "          ‘x’ and ‘y’ variables) to zero mean and unit variance.\n",
      "          The center and scale values are returned and used for later\n",
      "          predictions.\n",
      "\n",
      "    type: ‘svm’ can be used as a classification machine, as a\n",
      "          regression machine, or for novelty detection.  Depending of\n",
      "          whether ‘y’ is a factor or not, the default setting for\n",
      "          ‘type’ is ‘C-classification’ or ‘eps-regression’,\n",
      "          respectively, but may be overwritten by setting an explicit\n",
      "          value.\n",
      "          Valid options are:\n",
      "\n",
      "            • ‘C-classification’\n",
      "\n",
      "            • ‘nu-classification’\n",
      "\n",
      "            • ‘one-classification’ (for novelty detection)\n",
      "\n",
      "            • ‘eps-regression’\n",
      "\n",
      "            • ‘nu-regression’\n",
      "\n",
      "  kernel: the kernel used in training and predicting. You might\n",
      "          consider changing some of the following parameters, depending\n",
      "          on the kernel type.\n",
      "\n",
      "          linear: u'*v\n",
      "\n",
      "          polynomial: (gamma*u'*v + coef0)^degree\n",
      "\n",
      "          radial basis: exp(-gamma*|u-v|^2)\n",
      "\n",
      "          sigmoid: tanh(gamma*u'*v + coef0)\n",
      "\n",
      "  degree: parameter needed for kernel of type ‘polynomial’ (default:\n",
      "          3)\n",
      "\n",
      "   gamma: parameter needed for all kernels except ‘linear’ (default:\n",
      "          1/(data dimension))\n",
      "\n",
      "   coef0: parameter needed for kernels of type ‘polynomial’ and\n",
      "          ‘sigmoid’ (default: 0)\n",
      "\n",
      "    cost: cost of constraints violation (default: 1)-it is the\n",
      "          ‘C’-constant of the regularization term in the Lagrange\n",
      "          formulation.\n",
      "\n",
      "      nu: parameter needed for ‘nu-classification’,\n",
      "          ‘nu-regression’, and ‘one-classification’\n",
      "\n",
      "class.weights: a named vector of weights for the different classes,\n",
      "          used for asymmetric class sizes. Not all factor levels have\n",
      "          to be supplied (default weight: 1). All components have to be\n",
      "          named.\n",
      "\n",
      "cachesize: cache memory in MB (default 40)\n",
      "\n",
      "tolerance: tolerance of termination criterion (default: 0.001)\n",
      "\n",
      " epsilon: epsilon in the insensitive-loss function (default: 0.1)\n",
      "\n",
      "shrinking: option whether to use the shrinking-heuristics (default:\n",
      "          ‘TRUE’)\n",
      "\n",
      "   cross: if a integer value k>0 is specified, a k-fold cross\n",
      "          validation on the training data is performed to assess the\n",
      "          quality of the model: the accuracy rate for classification\n",
      "          and the Mean Squared Error for regression\n",
      "\n",
      "  fitted: logical indicating whether the fitted values should be\n",
      "          computed and included in the model or not (default: ‘TRUE’)\n",
      "\n",
      "probability: logical indicating whether the model should allow for\n",
      "          probability predictions.\n",
      "\n",
      "     ...: additional parameters for the low level fitting function\n",
      "          ‘svm.default’\n",
      "\n",
      "  subset: An index vector specifying the cases to be used in the\n",
      "          training sample.  (NOTE: If given, this argument must be\n",
      "          named.)\n",
      "\n",
      "na.action: A function to specify the action to be taken if ‘NA’s are\n",
      "          found. The default action is ‘na.omit’, which leads to\n",
      "          rejection of cases with missing values on any required\n",
      "          variable. An alternative is ‘na.fail’, which causes an\n",
      "          error if ‘NA’ cases are found. (NOTE: If given, this\n",
      "          argument must be named.)\n",
      "\n",
      "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
      "\n",
      "     For multiclass-classification with k levels, k>2, ‘libsvm’ uses\n",
      "     the ‘one-against-one’-approach, in which k(k-1)/2 binary\n",
      "     classifiers are trained; the appropriate class is found by a\n",
      "     voting scheme.\n",
      "\n",
      "     ‘libsvm’ internally uses a sparse data representation, which is\n",
      "     also high-level supported by the package ‘SparseM’.\n",
      "\n",
      "     If the predictor variables include factors, the formula interface\n",
      "     must be used to get a correct model matrix.\n",
      "\n",
      "     ‘plot.svm’ allows a simple graphical visualization of\n",
      "     classification models.\n",
      "\n",
      "     The probability model for classification fits a logistic\n",
      "     distribution using maximum likelihood to the decision values of\n",
      "     all binary classifiers, and computes the a-posteriori class\n",
      "     probabilities for the multi-class problem using quadratic\n",
      "     optimization. The probabilistic regression model assumes\n",
      "     (zero-mean) laplace-distributed errors for the predictions, and\n",
      "     estimates the scale parameter using maximum likelihood.\n",
      "\n",
      "_\bV_\ba_\bl_\bu_\be:\n",
      "\n",
      "     An object of class ‘\"svm\"’ containing the fitted model,\n",
      "     including:\n",
      "\n",
      "      SV: The resulting support vectors (possibly scaled).\n",
      "\n",
      "   index: The index of the resulting support vectors in the data\n",
      "          matrix. Note that this index refers to the preprocessed data\n",
      "          (after the possible effect of ‘na.omit’ and ‘subset’)\n",
      "\n",
      "   coefs: The corresponding coefficients times the training labels.\n",
      "\n",
      "     rho: The negative intercept.\n",
      "\n",
      "   sigma: In case of a probabilistic regression model, the scale\n",
      "          parameter of the hypothesized (zero-mean) laplace\n",
      "          distribution estimated by maximum likelihood.\n",
      "\n",
      "probA, probB: numeric vectors of length k(k-1)/2, k number of classes,\n",
      "          containing the parameters of the logistic distributions\n",
      "          fitted to the decision values of the binary classifiers (1 /\n",
      "          (1 + exp(a x + b))).\n",
      "\n",
      "_\bN_\bo_\bt_\be:\n",
      "\n",
      "     Data are scaled internally, usually yielding better results.\n",
      "\n",
      "     Parameters of SVM-models usually _must_ be tuned to yield sensible\n",
      "     results!\n",
      "\n",
      "_\bA_\bu_\bt_\bh_\bo_\br(_\bs):\n",
      "\n",
      "     David Meyer (based on C/C++-code by Chih-Chung Chang and Chih-Jen\n",
      "     Lin)\n",
      "     <email: David.Meyer@R-project.org>\n",
      "\n",
      "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
      "\n",
      "        • Chang, Chih-Chung and Lin, Chih-Jen:\n",
      "          _LIBSVM: a library for Support Vector Machines_\n",
      "          <URL: http://www.csie.ntu.edu.tw/~cjlin/libsvm>\n",
      "\n",
      "        • Exact formulations of models, algorithms, etc. can be found\n",
      "          in the document:\n",
      "          Chang, Chih-Chung and Lin, Chih-Jen:\n",
      "          _LIBSVM: a library for Support Vector Machines_\n",
      "          <URL: http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.ps.gz>\n",
      "\n",
      "        • More implementation details and speed benchmarks can be found\n",
      "          on: Rong-En Fan and Pai-Hsune Chen and Chih-Jen Lin:\n",
      "          _Working Set Selection Using the Second Order Information for\n",
      "          Training SVM_\n",
      "          <URL:\n",
      "          http://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf>\n",
      "\n",
      "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
      "\n",
      "     ‘predict.svm’ ‘plot.svm’ ‘tune.svm’ ‘matrix.csr’ (in\n",
      "     package ‘SparseM’)\n",
      "\n",
      "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
      "\n",
      "     data(iris)\n",
      "     attach(iris)\n",
      "     \n",
      "     ## classification mode\n",
      "     # default with factor response:\n",
      "     model <- svm(Species ~ ., data = iris)\n",
      "     \n",
      "     # alternatively the traditional interface:\n",
      "     x <- subset(iris, select = -Species)\n",
      "     y <- Species\n",
      "     model <- svm(x, y) \n",
      "     \n",
      "     print(model)\n",
      "     summary(model)\n",
      "     \n",
      "     # test with train data\n",
      "     pred <- predict(model, x)\n",
      "     # (same as:)\n",
      "     pred <- fitted(model)\n",
      "     \n",
      "     # Check accuracy:\n",
      "     table(pred, y)\n",
      "     \n",
      "     # compute decision values and probabilities:\n",
      "     pred <- predict(model, x, decision.values = TRUE)\n",
      "     attr(pred, \"decision.values\")[1:4,]\n",
      "     \n",
      "     # visualize (classes by color, SV by crosses):\n",
      "     plot(cmdscale(dist(iris[,-5])),\n",
      "          col = as.integer(iris[,5]),\n",
      "          pch = c(\"o\",\"+\")[1:150 %in% model$index + 1])\n",
      "     \n",
      "     ## try regression mode on two dimensions\n",
      "     \n",
      "     # create data\n",
      "     x <- seq(0.1, 5, by = 0.05)\n",
      "     y <- log(x) + rnorm(x, sd = 0.2)\n",
      "     \n",
      "     # estimate model and predict input values\n",
      "     m   <- svm(x, y)\n",
      "     new <- predict(m, x)\n",
      "     \n",
      "     # visualize\n",
      "     plot(x, y)\n",
      "     points(x, log(x), col = 2)\n",
      "     points(x, new, col = 4)\n",
      "     \n",
      "     ## density-estimation\n",
      "     \n",
      "     # create 2-dim. normal with rho=0:\n",
      "     X <- data.frame(a = rnorm(1000), b = rnorm(1000))\n",
      "     attach(X)\n",
      "     \n",
      "     # traditional way:\n",
      "     m <- svm(X, gamma = 0.1)\n",
      "     \n",
      "     # formula interface:\n",
      "     m <- svm(~., data = X, gamma = 0.1)\n",
      "     # or:\n",
      "     m <- svm(~ a + b, gamma = 0.1)\n",
      "     \n",
      "     # test:\n",
      "     newdata <- data.frame(a = c(0, 4), b = c(0, 4))\n",
      "     predict (m, newdata)\n",
      "     \n",
      "     # visualize:\n",
      "     plot(X, col = 1:1000 %in% m$index + 1, xlim = c(-5,5), ylim=c(-5,5))\n",
      "     points(newdata, pch = \"+\", col = 2, cex = 5)\n",
      "     \n",
      "     # weights: (example not particularly sensible)\n",
      "     i2 <- iris\n",
      "     levels(i2$Species)[3] <- \"versicolor\"\n",
      "     summary(i2$Species)\n",
      "     wts <- 100 / table(i2$Species)\n",
      "     wts\n",
      "     m <- svm(Species ~ ., data = i2, class.weights = wts)\n",
      "     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "R\"?svm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.10588396653408312\n",
      "2: 0.10026059525442327\n",
      "4: 0.08942531888629818\n",
      "6: 0.09107118365107666\n",
      "8: 0.07433822520916199\n"
     ]
    }
   ],
   "source": [
    "using RCall, DataFrames\n",
    "R\"library(e1071)\"\n",
    "\n",
    "function question2()\n",
    "    for i in 0:2:8\n",
    "        labels = n_vs_all(train_labels, i)\n",
    "        df = DataFrame([train_features labels])\n",
    "        R\"df <- $df\"\n",
    "        R\"df$x3 <- as.factor(df$x3)\"\n",
    "        R\"clf <- svm(x3~x1+x2, data=df, type='C-classification', cost=0.01, kernel='polynomial', degree=2, gamma=1.0, coef0=1.0, scale=FALSE)\"\n",
    "        #R\"plot(clf, df)\"\n",
    "        pred = rcopy(R\"fitted(clf)\")\n",
    "        pred = map(x->parse(Int32,x), pred)\n",
    "        println(i, \": \", sum(pred .!= labels)/length(labels))\n",
    "    end\n",
    "end\n",
    "\n",
    "question2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendices C: MATLAB.jl\n",
    "We can use MATLAB's functionality from Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A MATLAB session is open successfully\n",
      "0: 0.10588396653408312\n",
      "2: 0.10026059525442327\n",
      "4: 0.08942531888629818\n",
      "6: 0.09107118365107666\n",
      "8: 0.07433822520916199\n"
     ]
    }
   ],
   "source": [
    "using MATLAB\n",
    "\n",
    "function question2()\n",
    "    for i in 0:2:8\n",
    "        labels = n_vs_all(train_labels, i)\n",
    "        \n",
    "        mat\"\"\"\n",
    "        x = $train_features;\n",
    "        y = $labels;\n",
    "        clf = fitcsvm(x, y, 'BoxConstraint', 0.01, 'KernelFunction', 'polynomial','PolynomialOrder', 2);\n",
    "        $pred = predict(clf, x);   \n",
    "        \"\"\"\n",
    "        println(i, \": \", sum(pred .!= labels)/length(labels))\n",
    "    end\n",
    "end\n",
    "\n",
    "question2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.4",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
